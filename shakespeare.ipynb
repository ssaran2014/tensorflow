{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "shakespeare.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOMaG33eVXXpQtIUTeeknz9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ssaran2014/tensorflow/blob/master/shakespeare.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BieJIDwAPZgH",
        "colab_type": "text"
      },
      "source": [
        "The purpose of this notebook is to generate a somewhat relevant stream of words, given a context, using a recurrent neural network. The context here is the full works of Shakespeare. \n",
        "\n",
        "There are several approaches. The two most common ones are to predict character by character or to predict word by word. This notebook does the latter. The former approach can be seen here: https://www.tensorflow.org/tutorials/text/text_generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIvq0q7tJTAO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAP1BzOsJ-ne",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#download the complete works os Shakespeare\n",
        "#http://www.gutenberg.org/files/100/100-0.txt\n",
        "\n",
        "def load_data():\n",
        "  url = 'http://www.gutenberg.org/files/100/100-0.txt'\n",
        "\n",
        "  path_to_file = tf.keras.utils.get_file('shakespeare.txt', url)\n",
        "\n",
        "  r = open(path_to_file, 'rb').read().decode(encoding='utf-8').replace('\\r', '').lower().split('\\n')\n",
        "\n",
        "  return r\n",
        "\n",
        "cleaned_data = load_data()"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUgaqTGT1fdr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_subset(master_list, subset = 20000):\n",
        "  return master_list[:subset]\n",
        "subset_list = create_subset(cleaned_data)\n",
        "#len(subset_list)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhTa0UZ3QB-T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c14a4ab9-068b-4618-ada9-5cf832f5cf5d"
      },
      "source": [
        "def create_word_index(cleaned_data):\n",
        "  tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(cleaned_data)\n",
        "  total_words = len(tokenizer.word_index) + 1\n",
        "  print('Total number of words: {}'.format(total_words))\n",
        "  return tokenizer, total_words\n",
        "\n",
        "tokenizer, total_words = create_word_index(subset_list)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of words: 9589\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvHGvJH8PbXQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess(cleaned_data, total_words, tokenizer):\n",
        "\n",
        "  #converting each line to a sequence of numbers based on the word index\n",
        "  input_sequences = []\n",
        "  for line in cleaned_data:\n",
        "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "    #removing blank lines\n",
        "    if len(token_list)>0: \n",
        "      for i in range(1, len(token_list)):\n",
        "        input_sequences.append(token_list[:i+1])\n",
        "\n",
        "  #padding the sequences \n",
        "  max_sequence_len = max([len(x) for x in input_sequences])\n",
        "  input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "\n",
        "  # create predictors and label\n",
        "  xs, labels = input_sequences[:,:-1], input_sequences[:,-1]\n",
        "\n",
        "  #convert labels to categorical (numpy array)\n",
        "  ys = tf.keras.utils.to_categorical(labels, num_classes=total_words)\n",
        "\n",
        "  return xs, ys, max_sequence_len\n",
        "\n",
        "xs, ys, max_sequence_len = preprocess(subset_list, total_words, tokenizer)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KK66xWbhlBbr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "ecd155e7-b37d-4963-fd41-8c0aeacc251b"
      },
      "source": [
        "#define model\n",
        "def define_model(total_words, max_sequence_len):\n",
        "  embedding_dim = 32\n",
        "  input_length = max_sequence_len - 1\n",
        "\n",
        "  model = tf.keras.Sequential([\n",
        "                               tf.keras.layers.Embedding(total_words, embedding_dim, input_length = input_length),\n",
        "                               tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(150)),\n",
        "                               tf.keras.layers.Dense(total_words, activation = 'softmax')\n",
        "  ])\n",
        "\n",
        "  model.summary()\n",
        "\n",
        "  model.compile(\n",
        "      optimizer = 'adam',\n",
        "      loss = 'categorical_crossentropy',\n",
        "      metrics = ['accuracy']\n",
        "  )\n",
        "\n",
        "  return model\n",
        "\n",
        "model = define_model(total_words, max_sequence_len)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 17, 32)            306848    \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 300)               219600    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 9589)              2886289   \n",
            "=================================================================\n",
            "Total params: 3,412,737\n",
            "Trainable params: 3,412,737\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8GPqFr3v7u2j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4337430e-6ec4-475d-ef50-04f6986d2790"
      },
      "source": [
        "def history(model, xs, ys):\n",
        "  history = model.fit(xs, ys,\n",
        "                      epochs = 150,\n",
        "                      verbose = 1)\n",
        "  return history\n",
        "\n",
        "history = history(model, xs, ys)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            "2825/2825 [==============================] - 30s 11ms/step - loss: 6.8007 - accuracy: 0.0425\n",
            "Epoch 2/150\n",
            "2825/2825 [==============================] - 29s 10ms/step - loss: 6.2282 - accuracy: 0.0767\n",
            "Epoch 3/150\n",
            "2825/2825 [==============================] - 29s 10ms/step - loss: 5.8713 - accuracy: 0.0959\n",
            "Epoch 4/150\n",
            "2825/2825 [==============================] - 29s 10ms/step - loss: 5.5538 - accuracy: 0.1073\n",
            "Epoch 5/150\n",
            "2825/2825 [==============================] - 29s 10ms/step - loss: 5.2459 - accuracy: 0.1188\n",
            "Epoch 6/150\n",
            "2825/2825 [==============================] - 29s 10ms/step - loss: 4.9418 - accuracy: 0.1309\n",
            "Epoch 7/150\n",
            "2825/2825 [==============================] - 29s 10ms/step - loss: 4.6373 - accuracy: 0.1520\n",
            "Epoch 8/150\n",
            "2825/2825 [==============================] - 29s 10ms/step - loss: 4.3415 - accuracy: 0.1806\n",
            "Epoch 9/150\n",
            "2825/2825 [==============================] - 29s 10ms/step - loss: 4.0625 - accuracy: 0.2122\n",
            "Epoch 10/150\n",
            "2825/2825 [==============================] - 29s 10ms/step - loss: 3.8090 - accuracy: 0.2470\n",
            "Epoch 11/150\n",
            "2825/2825 [==============================] - 29s 10ms/step - loss: 3.5819 - accuracy: 0.2778\n",
            "Epoch 12/150\n",
            "2825/2825 [==============================] - 30s 11ms/step - loss: 3.3735 - accuracy: 0.3112\n",
            "Epoch 13/150\n",
            "2825/2825 [==============================] - 29s 10ms/step - loss: 3.1877 - accuracy: 0.3419\n",
            "Epoch 14/150\n",
            "2825/2825 [==============================] - 29s 10ms/step - loss: 3.0193 - accuracy: 0.3691\n",
            "Epoch 15/150\n",
            "2825/2825 [==============================] - 29s 10ms/step - loss: 2.8644 - accuracy: 0.3969\n",
            "Epoch 16/150\n",
            "2825/2825 [==============================] - 28s 10ms/step - loss: 2.7244 - accuracy: 0.4228\n",
            "Epoch 17/150\n",
            "2825/2825 [==============================] - 29s 10ms/step - loss: 2.5942 - accuracy: 0.4467\n",
            "Epoch 18/150\n",
            "2825/2825 [==============================] - 29s 10ms/step - loss: 2.4774 - accuracy: 0.4713\n",
            "Epoch 19/150\n",
            "2825/2825 [==============================] - 29s 10ms/step - loss: 2.3723 - accuracy: 0.4902\n",
            "Epoch 20/150\n",
            "2825/2825 [==============================] - 29s 10ms/step - loss: 2.2717 - accuracy: 0.5090\n",
            "Epoch 21/150\n",
            "2825/2825 [==============================] - 29s 10ms/step - loss: 2.1778 - accuracy: 0.5285\n",
            "Epoch 22/150\n",
            "2825/2825 [==============================] - 29s 10ms/step - loss: 2.0960 - accuracy: 0.5446\n",
            "Epoch 23/150\n",
            "2825/2825 [==============================] - 30s 11ms/step - loss: 2.0180 - accuracy: 0.5598\n",
            "Epoch 24/150\n",
            "2825/2825 [==============================] - 29s 10ms/step - loss: 1.9457 - accuracy: 0.5748\n",
            "Epoch 25/150\n",
            "2825/2825 [==============================] - 29s 10ms/step - loss: 1.8809 - accuracy: 0.5876\n",
            "Epoch 26/150\n",
            "2825/2825 [==============================] - 29s 10ms/step - loss: 1.8163 - accuracy: 0.6009\n",
            "Epoch 27/150\n",
            "2825/2825 [==============================] - 29s 10ms/step - loss: 1.7580 - accuracy: 0.6139\n",
            "Epoch 28/150\n",
            "2825/2825 [==============================] - 29s 10ms/step - loss: 1.7066 - accuracy: 0.6230\n",
            "Epoch 29/150\n",
            "2825/2825 [==============================] - 29s 10ms/step - loss: 1.6559 - accuracy: 0.6346\n",
            "Epoch 30/150\n",
            "2825/2825 [==============================] - 29s 10ms/step - loss: 1.6104 - accuracy: 0.6433\n",
            "Epoch 31/150\n",
            "2825/2825 [==============================] - 28s 10ms/step - loss: 1.5658 - accuracy: 0.6540\n",
            "Epoch 32/150\n",
            "2825/2825 [==============================] - 29s 10ms/step - loss: 1.5249 - accuracy: 0.6634\n",
            "Epoch 33/150\n",
            "2825/2825 [==============================] - 29s 10ms/step - loss: 1.4914 - accuracy: 0.6685\n",
            "Epoch 34/150\n",
            "2825/2825 [==============================] - 29s 10ms/step - loss: 1.4532 - accuracy: 0.6777\n",
            "Epoch 35/150\n",
            "2825/2825 [==============================] - 29s 10ms/step - loss: 1.4189 - accuracy: 0.6851\n",
            "Epoch 36/150\n",
            "2825/2825 [==============================] - 31s 11ms/step - loss: 1.3888 - accuracy: 0.6919\n",
            "Epoch 37/150\n",
            "2825/2825 [==============================] - 32s 11ms/step - loss: 1.3665 - accuracy: 0.6942\n",
            "Epoch 38/150\n",
            "2825/2825 [==============================] - 28s 10ms/step - loss: 1.3391 - accuracy: 0.7020\n",
            "Epoch 39/150\n",
            "2825/2825 [==============================] - 28s 10ms/step - loss: 1.3079 - accuracy: 0.7080\n",
            "Epoch 40/150\n",
            "2825/2825 [==============================] - 28s 10ms/step - loss: 1.2849 - accuracy: 0.7135\n",
            "Epoch 41/150\n",
            "2825/2825 [==============================] - 28s 10ms/step - loss: 1.2682 - accuracy: 0.7161\n",
            "Epoch 42/150\n",
            "2825/2825 [==============================] - 28s 10ms/step - loss: 1.2441 - accuracy: 0.7208\n",
            "Epoch 43/150\n",
            "2825/2825 [==============================] - 29s 10ms/step - loss: 1.2274 - accuracy: 0.7253\n",
            "Epoch 44/150\n",
            "2825/2825 [==============================] - 29s 10ms/step - loss: 1.2059 - accuracy: 0.7283\n",
            "Epoch 45/150\n",
            "2825/2825 [==============================] - 28s 10ms/step - loss: 1.1930 - accuracy: 0.7322\n",
            "Epoch 46/150\n",
            "2825/2825 [==============================] - 28s 10ms/step - loss: 1.1745 - accuracy: 0.7355\n",
            "Epoch 47/150\n",
            "2825/2825 [==============================] - 28s 10ms/step - loss: 1.1659 - accuracy: 0.7358\n",
            "Epoch 48/150\n",
            "2825/2825 [==============================] - 28s 10ms/step - loss: 1.1457 - accuracy: 0.7418\n",
            "Epoch 49/150\n",
            "2825/2825 [==============================] - 29s 10ms/step - loss: 1.1384 - accuracy: 0.7431\n",
            "Epoch 50/150\n",
            "2825/2825 [==============================] - 29s 10ms/step - loss: 1.1256 - accuracy: 0.7450\n",
            "Epoch 51/150\n",
            "2825/2825 [==============================] - 29s 10ms/step - loss: 1.1162 - accuracy: 0.7466\n",
            "Epoch 52/150\n",
            "2825/2825 [==============================] - 28s 10ms/step - loss: 1.1013 - accuracy: 0.7503\n",
            "Epoch 53/150\n",
            "2825/2825 [==============================] - 28s 10ms/step - loss: 1.0960 - accuracy: 0.7505\n",
            "Epoch 54/150\n",
            "2825/2825 [==============================] - 28s 10ms/step - loss: 1.0826 - accuracy: 0.7540\n",
            "Epoch 55/150\n",
            "2825/2825 [==============================] - 29s 10ms/step - loss: 1.0793 - accuracy: 0.7528\n",
            "Epoch 56/150\n",
            "2825/2825 [==============================] - 28s 10ms/step - loss: 1.0645 - accuracy: 0.7558\n",
            "Epoch 57/150\n",
            "2825/2825 [==============================] - 28s 10ms/step - loss: 1.0609 - accuracy: 0.7574\n",
            "Epoch 58/150\n",
            "2825/2825 [==============================] - 28s 10ms/step - loss: 1.0544 - accuracy: 0.7580\n",
            "Epoch 59/150\n",
            "2825/2825 [==============================] - 28s 10ms/step - loss: 1.0448 - accuracy: 0.7607\n",
            "Epoch 60/150\n",
            "2825/2825 [==============================] - 29s 10ms/step - loss: 1.0362 - accuracy: 0.7618\n",
            "Epoch 61/150\n",
            "2825/2825 [==============================] - 28s 10ms/step - loss: 1.0382 - accuracy: 0.7599\n",
            "Epoch 62/150\n",
            "2825/2825 [==============================] - 29s 10ms/step - loss: 1.0292 - accuracy: 0.7625\n",
            "Epoch 63/150\n",
            "2825/2825 [==============================] - 28s 10ms/step - loss: 1.0210 - accuracy: 0.7643\n",
            "Epoch 64/150\n",
            "2825/2825 [==============================] - 28s 10ms/step - loss: 1.0121 - accuracy: 0.7668\n",
            "Epoch 65/150\n",
            "2825/2825 [==============================] - 28s 10ms/step - loss: 1.0087 - accuracy: 0.7664\n",
            "Epoch 66/150\n",
            "2825/2825 [==============================] - 29s 10ms/step - loss: 1.0038 - accuracy: 0.7680\n",
            "Epoch 67/150\n",
            "2825/2825 [==============================] - 28s 10ms/step - loss: 1.0059 - accuracy: 0.7662\n",
            "Epoch 68/150\n",
            "2825/2825 [==============================] - 28s 10ms/step - loss: 0.9959 - accuracy: 0.7680\n",
            "Epoch 69/150\n",
            "2825/2825 [==============================] - 28s 10ms/step - loss: 0.9958 - accuracy: 0.7682\n",
            "Epoch 70/150\n",
            "2825/2825 [==============================] - 28s 10ms/step - loss: 0.9851 - accuracy: 0.7715\n",
            "Epoch 71/150\n",
            "2825/2825 [==============================] - 28s 10ms/step - loss: 0.9863 - accuracy: 0.7691\n",
            "Epoch 72/150\n",
            "2825/2825 [==============================] - 29s 10ms/step - loss: 0.9750 - accuracy: 0.7729\n",
            "Epoch 73/150\n",
            "2825/2825 [==============================] - 29s 10ms/step - loss: 0.9775 - accuracy: 0.7709\n",
            "Epoch 74/150\n",
            "2825/2825 [==============================] - 28s 10ms/step - loss: 0.9803 - accuracy: 0.7689\n",
            "Epoch 75/150\n",
            "2825/2825 [==============================] - 28s 10ms/step - loss: 0.9705 - accuracy: 0.7729\n",
            "Epoch 76/150\n",
            "2825/2825 [==============================] - 28s 10ms/step - loss: 0.9732 - accuracy: 0.7708\n",
            "Epoch 77/150\n",
            "2825/2825 [==============================] - 29s 10ms/step - loss: 0.9606 - accuracy: 0.7748\n",
            "Epoch 78/150\n",
            "2825/2825 [==============================] - 28s 10ms/step - loss: 0.9637 - accuracy: 0.7739\n",
            "Epoch 79/150\n",
            "2825/2825 [==============================] - 28s 10ms/step - loss: 0.9688 - accuracy: 0.7708\n",
            "Epoch 80/150\n",
            "2825/2825 [==============================] - 28s 10ms/step - loss: 0.9536 - accuracy: 0.7760\n",
            "Epoch 81/150\n",
            "2825/2825 [==============================] - 28s 10ms/step - loss: 0.9568 - accuracy: 0.7742\n",
            "Epoch 82/150\n",
            "2825/2825 [==============================] - 28s 10ms/step - loss: 0.9561 - accuracy: 0.7735\n",
            "Epoch 83/150\n",
            "2825/2825 [==============================] - 28s 10ms/step - loss: 0.9519 - accuracy: 0.7746\n",
            "Epoch 84/150\n",
            "2825/2825 [==============================] - 29s 10ms/step - loss: 0.9474 - accuracy: 0.7757\n",
            "Epoch 85/150\n",
            "2825/2825 [==============================] - 28s 10ms/step - loss: 0.9539 - accuracy: 0.7732\n",
            "Epoch 86/150\n",
            "2825/2825 [==============================] - 28s 10ms/step - loss: 0.9473 - accuracy: 0.7749\n",
            "Epoch 87/150\n",
            "2825/2825 [==============================] - 28s 10ms/step - loss: 0.9415 - accuracy: 0.7772\n",
            "Epoch 88/150\n",
            "2825/2825 [==============================] - 29s 10ms/step - loss: 0.9448 - accuracy: 0.7751\n",
            "Epoch 89/150\n",
            "2825/2825 [==============================] - 28s 10ms/step - loss: 0.9386 - accuracy: 0.7775\n",
            "Epoch 90/150\n",
            "2825/2825 [==============================] - 28s 10ms/step - loss: 0.9376 - accuracy: 0.7769\n",
            "Epoch 91/150\n",
            "2825/2825 [==============================] - 28s 10ms/step - loss: 0.9409 - accuracy: 0.7757\n",
            "Epoch 92/150\n",
            "2825/2825 [==============================] - 28s 10ms/step - loss: 0.9394 - accuracy: 0.7760\n",
            "Epoch 93/150\n",
            "2825/2825 [==============================] - 28s 10ms/step - loss: 0.9385 - accuracy: 0.7768\n",
            "Epoch 94/150\n",
            "2825/2825 [==============================] - 28s 10ms/step - loss: 0.9308 - accuracy: 0.7777\n",
            "Epoch 95/150\n",
            "2825/2825 [==============================] - 28s 10ms/step - loss: 0.9326 - accuracy: 0.7783\n",
            "Epoch 96/150\n",
            "2825/2825 [==============================] - 28s 10ms/step - loss: 0.9298 - accuracy: 0.7779\n",
            "Epoch 97/150\n",
            "2825/2825 [==============================] - 28s 10ms/step - loss: 0.9312 - accuracy: 0.7775\n",
            "Epoch 98/150\n",
            "2825/2825 [==============================] - 28s 10ms/step - loss: 0.9278 - accuracy: 0.7775\n",
            "Epoch 99/150\n",
            "2825/2825 [==============================] - 29s 10ms/step - loss: 0.9304 - accuracy: 0.7773\n",
            "Epoch 100/150\n",
            "2825/2825 [==============================] - 28s 10ms/step - loss: 0.9236 - accuracy: 0.7791\n",
            "Epoch 101/150\n",
            "2825/2825 [==============================] - 28s 10ms/step - loss: 0.9292 - accuracy: 0.7785\n",
            "Epoch 102/150\n",
            "2825/2825 [==============================] - 29s 10ms/step - loss: 0.9259 - accuracy: 0.7786\n",
            "Epoch 103/150\n",
            "2825/2825 [==============================] - 28s 10ms/step - loss: 0.9239 - accuracy: 0.7778\n",
            "Epoch 104/150\n",
            "2825/2825 [==============================] - 28s 10ms/step - loss: 0.9226 - accuracy: 0.7792\n",
            "Epoch 105/150\n",
            "2825/2825 [==============================] - 28s 10ms/step - loss: 0.9228 - accuracy: 0.7777\n",
            "Epoch 106/150\n",
            "2825/2825 [==============================] - 28s 10ms/step - loss: 0.9212 - accuracy: 0.7774\n",
            "Epoch 107/150\n",
            "2825/2825 [==============================] - 28s 10ms/step - loss: 0.9208 - accuracy: 0.7783\n",
            "Epoch 108/150\n",
            "2825/2825 [==============================] - 28s 10ms/step - loss: 0.9141 - accuracy: 0.7804\n",
            "Epoch 109/150\n",
            "2825/2825 [==============================] - 28s 10ms/step - loss: 0.9135 - accuracy: 0.7809\n",
            "Epoch 110/150\n",
            "2825/2825 [==============================] - 29s 10ms/step - loss: 0.9146 - accuracy: 0.7796\n",
            "Epoch 111/150\n",
            "2825/2825 [==============================] - 28s 10ms/step - loss: 0.9155 - accuracy: 0.7792\n",
            "Epoch 112/150\n",
            "2825/2825 [==============================] - 29s 10ms/step - loss: 0.9179 - accuracy: 0.7780\n",
            "Epoch 113/150\n",
            "2825/2825 [==============================] - 28s 10ms/step - loss: 0.9105 - accuracy: 0.7804\n",
            "Epoch 114/150\n",
            "2825/2825 [==============================] - 28s 10ms/step - loss: 0.9096 - accuracy: 0.7804\n",
            "Epoch 115/150\n",
            "2825/2825 [==============================] - 28s 10ms/step - loss: 0.9154 - accuracy: 0.7786\n",
            "Epoch 116/150\n",
            "2825/2825 [==============================] - 28s 10ms/step - loss: 0.9098 - accuracy: 0.7806\n",
            "Epoch 117/150\n",
            "2825/2825 [==============================] - 28s 10ms/step - loss: 0.9102 - accuracy: 0.7803\n",
            "Epoch 118/150\n",
            "2825/2825 [==============================] - 28s 10ms/step - loss: 0.9093 - accuracy: 0.7791\n",
            "Epoch 119/150\n",
            "2825/2825 [==============================] - 29s 10ms/step - loss: 0.9077 - accuracy: 0.7793\n",
            "Epoch 120/150\n",
            "2825/2825 [==============================] - 29s 10ms/step - loss: 0.9125 - accuracy: 0.7784\n",
            "Epoch 121/150\n",
            "2825/2825 [==============================] - 29s 10ms/step - loss: 0.8986 - accuracy: 0.7825\n",
            "Epoch 122/150\n",
            "2825/2825 [==============================] - 28s 10ms/step - loss: 0.9105 - accuracy: 0.7790\n",
            "Epoch 123/150\n",
            "2825/2825 [==============================] - 28s 10ms/step - loss: 0.8977 - accuracy: 0.7824\n",
            "Epoch 124/150\n",
            "2825/2825 [==============================] - 28s 10ms/step - loss: 0.9057 - accuracy: 0.7814\n",
            "Epoch 125/150\n",
            "2825/2825 [==============================] - 28s 10ms/step - loss: 0.9025 - accuracy: 0.7817\n",
            "Epoch 126/150\n",
            "2825/2825 [==============================] - 28s 10ms/step - loss: 0.9028 - accuracy: 0.7811\n",
            "Epoch 127/150\n",
            "2825/2825 [==============================] - 28s 10ms/step - loss: 0.8978 - accuracy: 0.7824\n",
            "Epoch 128/150\n",
            "2825/2825 [==============================] - 28s 10ms/step - loss: 0.9041 - accuracy: 0.7804\n",
            "Epoch 129/150\n",
            "2825/2825 [==============================] - 28s 10ms/step - loss: 0.9055 - accuracy: 0.7796\n",
            "Epoch 130/150\n",
            "2825/2825 [==============================] - 28s 10ms/step - loss: 0.9113 - accuracy: 0.7768\n",
            "Epoch 131/150\n",
            "2825/2825 [==============================] - 28s 10ms/step - loss: 0.9032 - accuracy: 0.7804\n",
            "Epoch 132/150\n",
            "2825/2825 [==============================] - 29s 10ms/step - loss: 0.8995 - accuracy: 0.7815\n",
            "Epoch 133/150\n",
            "2825/2825 [==============================] - 28s 10ms/step - loss: 0.8990 - accuracy: 0.7821\n",
            "Epoch 134/150\n",
            "2825/2825 [==============================] - 28s 10ms/step - loss: 0.9058 - accuracy: 0.7793\n",
            "Epoch 135/150\n",
            "2825/2825 [==============================] - 28s 10ms/step - loss: 0.9024 - accuracy: 0.7798\n",
            "Epoch 136/150\n",
            "2825/2825 [==============================] - 28s 10ms/step - loss: 0.9075 - accuracy: 0.7785\n",
            "Epoch 137/150\n",
            "2825/2825 [==============================] - 28s 10ms/step - loss: 0.9033 - accuracy: 0.7793\n",
            "Epoch 138/150\n",
            "2825/2825 [==============================] - 28s 10ms/step - loss: 0.8937 - accuracy: 0.7823\n",
            "Epoch 139/150\n",
            "2825/2825 [==============================] - 28s 10ms/step - loss: 0.9036 - accuracy: 0.7789\n",
            "Epoch 140/150\n",
            "2825/2825 [==============================] - 28s 10ms/step - loss: 0.9063 - accuracy: 0.7778\n",
            "Epoch 141/150\n",
            "2825/2825 [==============================] - 28s 10ms/step - loss: 0.9063 - accuracy: 0.7789\n",
            "Epoch 142/150\n",
            "2825/2825 [==============================] - 29s 10ms/step - loss: 0.8984 - accuracy: 0.7807\n",
            "Epoch 143/150\n",
            "2825/2825 [==============================] - 29s 10ms/step - loss: 0.9000 - accuracy: 0.7809\n",
            "Epoch 144/150\n",
            "2825/2825 [==============================] - 28s 10ms/step - loss: 0.9017 - accuracy: 0.7787\n",
            "Epoch 145/150\n",
            "2825/2825 [==============================] - 28s 10ms/step - loss: 0.9069 - accuracy: 0.7774\n",
            "Epoch 146/150\n",
            "2825/2825 [==============================] - 28s 10ms/step - loss: 0.8977 - accuracy: 0.7808\n",
            "Epoch 147/150\n",
            "2825/2825 [==============================] - 28s 10ms/step - loss: 0.8971 - accuracy: 0.7802\n",
            "Epoch 148/150\n",
            "2825/2825 [==============================] - 29s 10ms/step - loss: 0.9045 - accuracy: 0.7783\n",
            "Epoch 149/150\n",
            "2825/2825 [==============================] - 29s 10ms/step - loss: 0.8949 - accuracy: 0.7813\n",
            "Epoch 150/150\n",
            "2825/2825 [==============================] - 28s 10ms/step - loss: 0.8962 - accuracy: 0.7800\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCRQwOllzMLi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "81aaf98e-79d2-4e3f-ef21-00e0a1835bf5"
      },
      "source": [
        "def plot_graphs(history, string):\n",
        "  plt.plot(history.history[string])\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(string)\n",
        "  plt.show()\n",
        "\n",
        "plot_graphs(history, 'accuracy')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Qi1znzQzpL4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "69f73fa6-8e8f-42fe-a75f-1119956acc7d"
      },
      "source": [
        "def create_play(tokenizer, model):\n",
        "  seed_text = \"if you survive this\"\n",
        "  next_words = 100\n",
        "\n",
        "  for _ in range(next_words):\n",
        "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "    predicted = np.argmax(model.predict(token_list)) #model.predict_classes(token_list, verbose=0)\n",
        "    output_word = \"\"\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "      if index == predicted:\n",
        "        output_word = word\n",
        "        break\n",
        "    seed_text += \" \" + output_word\n",
        "  print(seed_text)\n",
        "\n",
        "create_play(tokenizer, model)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "if you survive this more but i have no health with you with my house if i do not eros my lord but when i should not tell thee revenges as well of my worth of weeping ere i mangled do any most like of thee i will give a lordship by my life he says i when antonio the way of the gods blow self not hills not have hill to desired upon our own honour when he palace my lord the phoenix come no quality me in a villain i would make a will show me to he will when he at\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLhPTfOOJKEl",
        "colab_type": "text"
      },
      "source": [
        "A sequence of words is generated, but the sequence has no meaning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9tWy861exal",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def be_shakespeare():\n",
        "  #load full works of Shakespeare\n",
        "  cleaned_data = load_data()\n",
        "\n",
        "  #work with a subset of the data to manage memory usage\n",
        "  subset_list = create_subset(cleaned_data)\n",
        "\n",
        "  #tokenize and create word index\n",
        "  tokenizer, total_words = create_word_index(subset_list)\n",
        "  \n",
        "  #preprocess. Convert to sequences; pad sequences and convert ys to numpy array\n",
        "  xs, ys, max_sequence_len = preprocess(subset_list, total_words, tokenizer)\n",
        "\n",
        "  #define model\n",
        "  model = define_model(total_words, max_sequence_len)\n",
        "\n",
        "  #execute model and create history\n",
        "  history = history(model, xs, ys)\n",
        "\n",
        "  #plot graphs\n",
        "  plot_graphs(history, 'accuracy')\n",
        "\n",
        "  #make some words\n",
        "  create_play(tokenizer, model)\n",
        "\n",
        "be_shakespeare()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}